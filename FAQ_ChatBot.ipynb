{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiSrihitha-Mallela/GenAIChatbotForFAQs-G200-PS24/blob/main/FAQ_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVKFYSMQ4Tz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok flask transformers langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RALZsIZmREB7",
        "outputId": "97a1b1f3-15b8-4245-f84b-8726dc04b2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain)\n",
            "  Downloading langchain_core-0.3.25-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading pyngrok-7.2.2-py3-none-any.whl (22 kB)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.25-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n",
            "Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pyngrok, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.24\n",
            "    Uninstalling langchain-core-0.3.24:\n",
            "      Successfully uninstalled langchain-core-0.3.24\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.2\n",
            "    Uninstalling langchain-text-splitters-0.3.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.11\n",
            "    Uninstalling langchain-0.3.11:\n",
            "      Successfully uninstalled langchain-0.3.11\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.12 langchain-community-0.3.12 langchain-core-0.3.25 langchain-text-splitters-0.3.3 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 pyngrok-7.2.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm langchain PyMuPDF transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzWTlt4WRTLh",
        "outputId": "719c81e5-67ed-43f5-ef43-d1160139ee0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, PyMuPDF, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.25.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  langchain-community faiss-cpu"
      ],
      "metadata": {
        "id": "miCFKfcdRYnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple, Dict, Any\n",
        "from datasets import Dataset, load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import os\n",
        "\n",
        "# Set pandas display option\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", None  # This will be helpful when visualizing retriever outputs\n",
        ")"
      ],
      "metadata": {
        "id": "HSS5HozIRczJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting text from pdf\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using PyMuPDF.\n",
        "\n",
        "    Parameters:\n",
        "    - pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - str: The extracted text content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "l0Ua8TIWRc1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_dataset(\n",
        "    dataset_type: str,\n",
        "    data_files: Any,\n",
        "    split: str = \"train\",\n",
        "    delimiter: Optional[str] = None,\n",
        "    text_column: Optional[str] = None,\n",
        "    metadata_columns: Optional[List[str]] = None,\n",
        "    **kwargs\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Loads a dataset of the specified type and processes it into Langchain Documents.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset_type (str): The type of the dataset (e.g., 'text', 'csv', 'json', 'pdf', 'excel').\n",
        "    - data_files (str or dict): Path(s) to the data file(s).\n",
        "    - split (str): The dataset split to load. Default is 'train'.\n",
        "    - delimiter (str, optional): Delimiter for CSV files.\n",
        "    - text_column (str, optional): The column name that contains the text.\n",
        "    - metadata_columns (List[str], optional): List of column names to include as metadata.\n",
        "    - **kwargs: Additional keyword arguments for load_dataset.\n",
        "\n",
        "    Returns:\n",
        "    - List[LangchainDocument]: A list of Langchain Document objects.\n",
        "    \"\"\"\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    if dataset_type == \"pdf\":\n",
        "        # Handle PDF files separately\n",
        "        if isinstance(data_files, str):\n",
        "            data_files = [data_files]\n",
        "        for pdf_file in tqdm(data_files, desc=\"Processing PDF files\"):\n",
        "            text_content = extract_text_from_pdf(pdf_file)\n",
        "            if not text_content:\n",
        "                continue  # Skip if no text extracted\n",
        "            metadata = {}\n",
        "            if metadata_columns:\n",
        "                # Extract metadata from filename or other sources as needed\n",
        "                # Here, we'll use the filename as the source\n",
        "                metadata[\"source\"] = os.path.basename(pdf_file)\n",
        "                for col in metadata_columns:\n",
        "                    if col != \"source\":  # Avoid overwriting 'source'\n",
        "                        metadata[col] = \"\"  # Assign empty string or extract as needed\n",
        "            doc = LangchainDocument(page_content=text_content, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "    elif dataset_type == \"excel\":\n",
        "        # Handle Excel files\n",
        "        if isinstance(data_files, str):\n",
        "            data_files = [data_files]\n",
        "        for excel_file in tqdm(data_files, desc=\"Processing Excel files\"):\n",
        "            try:\n",
        "                df = pd.read_excel(excel_file)\n",
        "                for _, row in df.iterrows():\n",
        "                    page_content = row.get(text_column, \"\")\n",
        "                    if not page_content:\n",
        "                        continue\n",
        "                    metadata = {}\n",
        "                    if metadata_columns:\n",
        "                        for col in metadata_columns:\n",
        "                            metadata[col] = row.get(col, \"\")\n",
        "                    doc = LangchainDocument(page_content=page_content, metadata=metadata)\n",
        "                    documents.append(doc)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing Excel file {excel_file}: {e}\")\n",
        "                continue\n",
        "    elif dataset_type == \"json\":\n",
        "        # Handle JSON files\n",
        "        try:\n",
        "            ds = load_dataset(\n",
        "                dataset_type,\n",
        "                data_files=data_files,\n",
        "                split=split,\n",
        "                **kwargs\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            return []\n",
        "\n",
        "        for entry in tqdm(ds, desc=\"Processing JSON documents\"):\n",
        "            # Extract metadata\n",
        "            metadata = {}\n",
        "            if metadata_columns:\n",
        "                for col in metadata_columns:\n",
        "                    metadata[col] = entry.get(col, \"\")\n",
        "\n",
        "            # Extract and process the text from 'sections'\n",
        "            sections = entry.get(text_column, [])\n",
        "            if not sections:\n",
        "                continue  # Skip if no sections\n",
        "\n",
        "            # Concatenate all 'content' from sections into a single string\n",
        "            content_list = []\n",
        "            for section in sections:\n",
        "                heading = section.get(\"heading\", \"\")\n",
        "                content = section.get(\"content\", [])\n",
        "                if heading:\n",
        "                    content_list.append(f\"### {heading}\\n\")\n",
        "                if isinstance(content, list):\n",
        "                    content_list.extend(content)\n",
        "                elif isinstance(content, str):\n",
        "                    content_list.append(content)\n",
        "\n",
        "            # Join the content list into a single string\n",
        "            page_content = \"\\n\".join(content_list).strip()\n",
        "            if not page_content:\n",
        "                continue  # Skip if no content\n",
        "\n",
        "            # Create Langchain Document\n",
        "            doc = LangchainDocument(page_content=page_content, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "    else:\n",
        "        # Load the dataset using Hugging Face's load_dataset\n",
        "        try:\n",
        "            ds = load_dataset(\n",
        "                dataset_type,\n",
        "                data_files=data_files,\n",
        "                split=split,\n",
        "                delimiter=delimiter,\n",
        "                **kwargs\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            return []\n",
        "\n",
        "        # Determine the text column\n",
        "        if text_column is None:\n",
        "            if dataset_type == \"text\":\n",
        "                text_column = \"text\"\n",
        "            elif dataset_type in [\"csv\", \"parquet\"]:\n",
        "                # Attempt to infer the text column\n",
        "                sample = ds[0]\n",
        "                text_columns = [key for key, value in sample.items() if isinstance(value, str)]\n",
        "                if text_columns:\n",
        "                    text_column = text_columns[0]\n",
        "                    print(f\"Inferred text column as '{text_column}' for dataset type '{dataset_type}'\")\n",
        "                else:\n",
        "                    raise ValueError(\"No suitable text column found. Please specify 'text_column'.\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
        "\n",
        "        # Process each entry in the dataset\n",
        "        for entry in tqdm(ds, desc=f\"Processing {dataset_type} documents\"):\n",
        "            page_content = entry.get(text_column, \"\")\n",
        "            if not page_content:\n",
        "                continue  # Skip entries without text\n",
        "\n",
        "            # Prepare metadata\n",
        "            metadata = {}\n",
        "            if metadata_columns:\n",
        "                for col in metadata_columns:\n",
        "                    metadata[col] = entry.get(col, \"\")\n",
        "\n",
        "            # Create Langchain Document\n",
        "            doc = LangchainDocument(page_content=page_content, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "# Define dataset configurations\n",
        "dataset_configs = [\n",
        "    # {\n",
        "    #     \"dataset_type\": \"text\",\n",
        "    #     \"data_files\": \"Kmit_doc.rtf\",  # Replace with your actual file path\n",
        "    #     \"split\": \"train\",\n",
        "    #     \"text_column\": \"text\",  # Typically 'text' for text datasets\n",
        "    # },\n",
        "    # {\n",
        "    #     \"dataset_type\": \"csv\",\n",
        "    #     \"data_files\": \"/content/web_scraped_data_full.csv\",  # Replace with your actual CSV file path\n",
        "    #     \"split\": \"train\",\n",
        "    #     \"delimiter\": \",\",\n",
        "    #     \"text_column\": \"content\",  # Replace with your actual text column name\n",
        "    #     \"metadata_columns\": [\"source\", \"author\"]\n",
        "    # },\n",
        "    # {\n",
        "    #     \"dataset_type\": \"pdf\",\n",
        "    #     \"data_files\": [\"/content/administration.pdf\"],  # Replace with your actual PDF file paths\n",
        "    #     \"metadata_columns\": [\"source\"]  # Add any other metadata columns as needed\n",
        "    # },\n",
        "    # Add more configurations for different dataset types as needed\n",
        "    # Example for JSON:\n",
        "   {\n",
        "    \"dataset_type\": \"json\",\n",
        "    \"data_files\": \"/content/drive/MyDrive/KMIT_Dataset.json\",  # Replace with your actual JSON file path\n",
        "    \"split\": \"train\",\n",
        "    \"text_column\": \"sections\",  # Updated to match your JSON structure\n",
        "    \"metadata_columns\": [\"url\", \"title\"]  # Include relevant metadata fields\n",
        "   },\n",
        "    # {\n",
        "    # \"dataset_type\": \"excel\",\n",
        "    # \"data_files\": \"data.xlsx\",\n",
        "    # \"text_column\": \"Content\",  # Replace with your actual text column name\n",
        "    # \"metadata_columns\": [\"Source\", \"Author\"]\n",
        "    # },\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "-n1eVHF5Rc4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process datasets based on configurations\n",
        "RAW_KNOWLEDGE_BASE = []\n",
        "for config in dataset_configs:\n",
        "    docs = load_and_process_dataset(**config)\n",
        "    RAW_KNOWLEDGE_BASE.extend(docs)"
      ],
      "metadata": {
        "id": "RqVn2bVNRc7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the processed documents\n",
        "print(f\"Total documents loaded: {len(RAW_KNOWLEDGE_BASE)}\")\n",
        "for i, doc in enumerate(RAW_KNOWLEDGE_BASE[:2], 1):  # Display first 2 documents as a sample\n",
        "    print(f\"\\nDocument {i}:\")\n",
        "    print(f\"Content: {doc.page_content[:50000]}...\")  # Print first 500 characters for brevity\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Optional: Convert to a Pandas DataFrame for further analysis\n",
        "# df = pd.DataFrame([{\"content\": doc.page_content, **doc.metadata} for doc in RAW_KNOWLEDGE_BASE])\n",
        "# display(df.head())"
      ],
      "metadata": {
        "id": "rZs950L7Rc9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"  # get model here\n",
        "\n",
        "# Check model's max sequence length\n",
        "print(f\"Model's maximum sequence length: {SentenceTransformer(EMBEDDING_MODEL_NAME).max_seq_length}\")\n",
        "\n",
        "MARKDOWN_SEPERATORS = [\n",
        "    \"\\n#{1,6}\",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \".\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size=int,\n",
        "    knowledge_base=List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Splits a list of documents into smaller chunks using a specified tokenizer.\n",
        "    Split document into chunks of maximum size 'chunk_size' tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPERATORS,\n",
        "    )\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove Duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# Tokenized chunks\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n",
        "\n",
        "# Visualize chunk sizes in tokens\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "print(\"NO OF TOKENS IN EACH CHUNK: \", lengths, \"\\n\") #the number of tokens in each chunk\n",
        "\n",
        "print(\"TOTAL TOKENS: \", sum(lengths), \"\\n\") #the total number of tokens across all documents\n",
        "\n",
        "print(\"TOTAL CHUNKS:\" ,len(docs_processed), \"\\n\") #the total number of chunks across all documents\n",
        "\n",
        "print(\"FIRST 3 CHUNKS FOR INSPECTION: \", docs_processed[:3] ,\"\\n\") #This will show the first 10 chunks for inspection.\n",
        "\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zDGY-IjJRdBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "#CREATING AN EMBBEDING MODEL\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name = EMBEDDING_MODEL_NAME,\n",
        "    multi_process = True,\n",
        "    model_kwargs = {\"device\" : \"cuda\"},\n",
        "    encode_kwargs = {\"normalize_embeddings\" : True}, #set True for cosine similarity\n",
        ")\n",
        "\n",
        "#CREATING A VECTOR DATABASE\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy = DistanceStrategy.COSINE\n",
        ")"
      ],
      "metadata": {
        "id": "G3Koe4OAR2gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM or generator of rag**"
      ],
      "metadata": {
        "id": "ZUprN4cpSBEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "auth_token = \"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\"\n",
        "\n",
        "# Load the model and apply dynamic quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token = auth_token)\n",
        "# model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token = auth_token)\n",
        "\n",
        "# Now you can use this model in the pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    device=\"cuda\",\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ],
      "metadata": {
        "id": "h5YWDY3wR28v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an AI assistant designed to engage in natural human conversation and provide accurate and concise answers to queries about Keshav Memorial Institute of Technology (KMIT).\n",
        "                      You should respond to basic greetings in a friendly and polite manner and understand the flow of human conversation.\n",
        "                      Your tone should always remain friendly, polite, and professional.\n",
        "                      For farewells such as 'bye,' 'goodbye,' or similar phrases, reply with a friendly closing like 'Goodbye! It was nice talking to you. Have a great day!'\n",
        "                      Provide answers only based on the information you have been given or trained on. Under no circumstances should you mention or reference the source of your information or document.\n",
        "                      If you do not know the answer to a query or if the query is unrelated to KMIT, politely inform the user that you cannot provide an answer. If relevant, recommend visiting the official college website for more information. Avoid guessing, fabricating, or providing unrelated responses based on context.\n",
        "                      Ensure that you handle all queries in a case-insensitive manner, treating uppercase and lowercase text equally.\n",
        "                      Keep your responses brief, accurate, and conversational unless the user requests further clarification or detail.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "        {context}\n",
        "        ---\n",
        "        Now here is the question you need to answer.\n",
        "        Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize = False, add_generation_prompt = True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "mTph7_QuR3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate ngrok using your token\n",
        "ngrok.set_auth_token(\"2irTQMNkUZMcYuVcTGj09nHHU8F_64b8QmF2kHdqL9ZQ6HGG6\")"
      ],
      "metadata": {
        "id": "VjF7rO2lR3DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/process-query', methods=['POST'])\n",
        "def process_query():\n",
        "    try:\n",
        "      data = request.get_json()\n",
        "      user_query = data.get('query')\n",
        "      print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "      retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query = user_query, k=2)\n",
        "\n",
        "      retrieved_docs_text = [doc.page_content for doc in retrieved_docs]\n",
        "       #we only need the text of the documents\n",
        "      context = '\\nExtracted documents:\\n'\n",
        "      context += \"\".join(\n",
        "          [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        "      )\n",
        "      # print(\"CONTEXT: \", context);\n",
        "      final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "          question = user_query, context =context\n",
        "      )\n",
        "\n",
        "      #redact an answer\n",
        "      answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "      print(\"Answer: \", answer)\n",
        "\n",
        "      return jsonify({\"message\":answer})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from pyngrok import ngrok\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"Public URL:\", public_url)\n",
        "    app.run(port=5000)\n",
        "# user_query = \"What are the projects driven Technologies taught to II year students?\"\n"
      ],
      "metadata": {
        "id": "Zt3HpCW-R3GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to Run and connect the model with backend**\n",
        "1) Run all cells\n",
        "2) Run server 3) Run RAG 4) Get the ngrok url and paste it in the backend 5) Run the backend again (Just make any small change in any console.log statement it will rerun automatically) 6) Now they are connected"
      ],
      "metadata": {
        "id": "ceTxrVQeSR1N"
      }
    }
  ]
}